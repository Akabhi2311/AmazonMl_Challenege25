ğŸš€ Amazon Summer Challenge 2025 â€“ Image Classification (Top 10%)

This repository contains my end-to-end solution for the Amazon Summer Challenge 2025, where participants were given a 75,000-image training dataset with associated feature metadata and were required to predict labels for an additional 75,000 unlabeled test images.

I achieved a Top 10% leaderboard ranking by building a multimodal (image + tabular) deep learning pipeline optimized for large-scale inference.

ğŸ§© Problem Statement

Amazon encounters millions of product images uploaded daily. The challenge simulated a real workflow where participants needed to:

Build an image classification model using
âœ” 75K labeled training images
âœ” 75K unlabeled test images
âœ” Metadata/features for each image (tabular inputs)

Predict the correct class for each test image

Generate a submission file in the required format

Optimize for both accuracy and inference performance

ğŸ“ Dataset Details

The dataset was divided into:

dataset/
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_images/            # 75,000 images
â”‚   â”œâ”€â”€ train.csv                # metadata + labels
â”‚
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_images/             # 75,000 images
â”‚   â”œâ”€â”€ test.csv                 # metadata only
â”‚
â””â”€â”€ sample_submission.csv

Metadata Included:

Numerical features

Categorical attributes

Pre-extracted product information

Label (train.csv only)

ğŸ›  Approach
1. Image Preprocessing

Resizing images (224Ã—224)

Normalization

Augmentations: rotations, flips, color jitter

Loaded efficiently using PyTorch DataLoader

2. Tabular Feature Engineering

Missing value imputation

Encoding categorical fields

Normalization of continuous features

Feature interaction + frequency encoding

3. Model Architecture

A multimodal fusion model:

ğŸ”¹ Vision Branch

Pretrained CNN (ResNet50 / EfficientNet)

Extracted 512â€“1024D image embeddings

ğŸ”¹ Tabular Branch

2â€“3 layer MLP

ReLU + BatchNorm + Dropout

ğŸ”¹ Fusion Layer

Concatenation of both embeddings

Dense layers â†’ Softmax output

4. Training

Loss: CrossEntropy

Optimizer: AdamW

Scheduler: Cosine with Warmup

Trained for 15â€“25 epochs

Early stopping on Macro F1

5. Evaluation Metric

Macro F1 Score
(Used due to class imbalance.)

ğŸ† Results

Ranked in the Top 10% of all teams

Successfully handled 150K images end-to-end

Achieved strong generalization using a multimodal architecture

Built a scalable inference pipeline

ğŸ“‚ Repository Structure
Amazon-Summer-Challenge-2025/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_images/
â”‚   â”œâ”€â”€ test_images/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ dataset_loader.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ utils.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb
â”‚   â”œâ”€â”€ Training.ipynb
â”‚   â”œâ”€â”€ Image_Feature_Fusion.ipynb
â”‚
â”œâ”€â”€ submission/
â”‚   â””â”€â”€ submission.csv
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš¡ How to Run
Install dependencies:
pip install -r requirements.txt

Preprocess the dataset:
python src/preprocess.py

Train the model:
python src/train_model.py

Generate predictions:
python src/inference.py

ğŸ¯ Key Highlights

ğŸ“Œ Multimodal ML: Vision + Tabular fusion

ğŸ“Œ Handles large datasets (150K images)

ğŸ“Œ Clean inference pipeline for fast batch predictions

ğŸ“Œ Reproducible code and modular design

ğŸ“œ License

MIT License â€” feel free to use and modify.
